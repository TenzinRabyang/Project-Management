{"paragraphs":[{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 1: EXTRACT DATA FROM HDFS TO SPARK\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#Section 1.1 - Loading UNEMPLOYMENT CSV from Maria_Dev HDFS Location\nunemp <- read.df(sqlContext, \"/user/maria_dev/DataStaging/UnemploymentRate_YorkAll.csv\", \"com.databricks.spark.csv\", header=\"FALSE\", inferSchema = \"true\")\n\n#Assign appropriate column names to data frame\ncolnames(unemp)=c('TimePeriod','UnemploymentRate')\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#Section 1.2 - Loading UK CRIME STATISTICS CSV from Maria_Dev HDFS Location\nukc <- read.df(sqlContext, \"/user/maria_dev/DataStaging/UKCrimeStats.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\n\n#Assign appropriate column names to data frame\ncolnames(ukc)=c('Country','Year', 'Month', 'NoofCrimes')\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#Section 1.3 - Loading Stop Search CSV from Maria_Dev HDFS Location\nssf <- read.df(sqlContext, \"/user/maria_dev/DataStaging/StopSearch_Final.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#Section 1.4 - Loading Street Crime CSV from Maria_Dev HDFS Location\nscf1 <- read.df(sqlContext, \"/user/maria_dev/DataStaging/Street_Final_1.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\nscf2 <- read.df(sqlContext, \"/user/maria_dev/DataStaging/Street_Final_2.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\n\n#HDFS limited to 200MB file upload. Actual file is ~375MB. Hence, File is split and appended in Spark\nscf=rbind(scf1,scf2)\n\nrm(scf1,scf2)\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#Section 1.5 - Loading Outcome CSV from Maria_Dev HDFS Location\noc1 <- read.df(sqlContext, \"/user/maria_dev/DataStaging/Outcome_Final_1.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\noc2 <- read.df(sqlContext, \"/user/maria_dev/DataStaging/Outcome_Final_2.csv\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\n\n#HDFS limited to 200MB file upload. Actual file is ~375MB. Hence, File is split and appended in Spark\noc=rbind(oc1, oc2)\n\nrm(oc1, oc2)\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------","user":"anonymous","dateUpdated":"2022-04-19T15:02:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=279","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=280","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=281","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=282","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=283","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=284","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=285","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=286","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=287","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=288","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=289","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=290","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=291","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=292"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1649911271368_53390541","id":"20220414-044111_247185164","dateCreated":"2022-04-14T04:41:11+0000","dateStarted":"2022-04-19T15:02:26+0000","dateFinished":"2022-04-19T15:02:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11541"},{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 2: TRANSFORMATION STAGE - UNEMPLOYMENT DATAFRAME\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#DATA VALIDITY - RECORDS BEFORE APPLYING TRANSFORMATIONS\npaste0('RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: ',nrow(unemp),' COLUMNS: ',ncol(unemp))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: EXTRACT DATE COLUMNS\n\n#USE SUBSTRING FUNCTION TO EXTRACT DATE COLUMNS\nunemp$Year = substr(unemp$TimePeriod, 1, 4)\nunemp$Monthname = substr(unemp$TimePeriod, 6, 8)\n\n#CONVERT MMM STRING TO NUMERIC MONTH (MM) COLUMN\nunemp$Month = ifelse(unemp$Monthname=='JAN', 1, \n                    ifelse(unemp$Monthname=='FEB', 2,\n                        ifelse(unemp$Monthname=='MAR', 3,\n                            ifelse(unemp$Monthname=='APR', 4,\n                                ifelse(unemp$Monthname=='MAY', 5,\n                                    ifelse(unemp$Monthname=='JUN', 6,\n                                        ifelse(unemp$Monthname=='JUL', 7,\n                                            ifelse(unemp$Monthname=='AUG', 8,\n                                                ifelse(unemp$Monthname=='SEP', 9,\n                                                    ifelse(unemp$Monthname=='OCT', 10,\n                                                        ifelse(unemp$Monthname=='NOV', 11, 12)))))))))))\n\n#REMOVE ORIGINAL COLUMNS WHICH ARE NOT NEEDED\nunemp$TimePeriod = NULL\nunemp$Monthname = NULL\n\n#DATA VALIDITY - CHECK DATA TYPE OF EACH COLUMN\nprint('DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN')\nstr(unemp)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: CONVERT COLUMNS DATA TYPES \nunemp$Year <- SparkR::cast(unemp$Year, \"double\")\n\n#DATA TRANSFORMATION: APPLY FILTER FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020\nunemp=subset(unemp, unemp$Year >= 2020)\n\n#DATA VALIDITY - RECORDS AFTER APPLYING FILTER\npaste0('RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: ',nrow(unemp),' COLUMNS: ',ncol(unemp))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA FOR INCONSISTENCY\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY')\nshowDF(count(groupBy(unemp, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(unemp, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(unemp, 'UnemploymentRate'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK NULL VALUES IN EACH COLUMN IN DATA FRAME\npaste(\"NUMBER OF NULL RECORDS IN YEAR COLUMN IS: \",nrow(SparkR::filter(unemp, isNull(unemp$Year))))\npaste(\"NUMBER OF NULL RECORDS IN MONTH COLUMN IS: \",nrow(SparkR::filter(unemp, isNull(unemp$Month))))\npaste(\"NUMBER OF NULL RECORDS IN UNEMPLOYMENTRATE COLUMN IS: \",nrow(SparkR::filter(unemp, isNull(unemp$UnemploymentRate))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: NULL VALUE TREATMENT\nunemp$Year = ifelse(isNull(unemp$Year)==TRUE, -99, unemp$Year)\nunemp$Month = ifelse(isNull(unemp$Month)==TRUE, -99, unemp$Month)\nunemp$UnemploymentRate = ifelse(isNull(unemp$UnemploymentRate)==TRUE, -99, unemp$UnemploymentRate)\n\n#DATA TRANSFORMATION: INVALID VALUE TREATMENT\n#NO INVALID VALUES DETECTED --> BY FREQUENCY DISTRIBUTION\n#NO BLANK/SPACE DETECTED --> BY FREQUENCY DISTRIBUTION\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA AFTER NULL/INVALID DATA TREATMENT\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED')\nshowDF(count(groupBy(unemp, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(unemp, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(unemp, 'UnemploymentRate'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK DUPLICATE RECORDS\nprint('DATA VALIDITY - CHECK DUPLICATE RECORDS')\npaste0(\"TOTAL RECORDS IN UNEMPLOYMENT DATAFRAME: \",nrow(unemp))\npaste0(\"DUPLICATE RECORDS IN UNEMPLOYMENT DATAFRAME: \",(nrow(unemp)-nrow(collect(distinct(unemp)))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: REMOVE DUPLICATE RECORDS\nunemp=distinct(unemp)\n\n#DATA VALIDITY - RECORDS AFTER DUPLICATES REMOVAL\npaste0('RECORDS AFTER DUPLICATES REMOVAL - ROWS: ',nrow(unemp),' COLUMNS: ',ncol(unemp))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - FINAL UNEMPLOYMENT TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE\npaste0('FINAL UNEMPLOYMENT TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: ',nrow(unemp),' COLUMNS: ',ncol(unemp))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(unemp, \"unemp\")\n\nprint(\"TRANSFORMATION OF UNEMPLOYMENT DATAFRAME IS COMPLETED\")","user":"anonymous","dateUpdated":"2022-04-19T12:24:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[1] “RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: 357 COLUMNS: 2”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN”\n'SparkDataFrame': 3 variables:\n $ UnemploymentRate: num 10.2 9.6 9.5 8.8 9.1 9.4\n $ Year            : chr “1992” “1992” “1992” “1992” “1992” “1992”\n $ Month           : num 4 5 6 7 8 9\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: 24 COLUMNS: 3”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY”\n+——+—–+\n|  Year|count|\n+——+—–+\n|2020.0|   12|\n|2021.0|   12|\n+——+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|  8.0|    2|\n|  7.0|    2|\n|  1.0|    2|\n|  4.0|    2|\n| 11.0|    2|\n|  3.0|    2|\n|  2.0|    2|\n| 10.0|    2|\n|  6.0|    2|\n|  5.0|    2|\n|  9.0|    2|\n| 12.0|    2|\n+—–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|  UnemploymentRate|\n+——-+——————+\n|  count|                24|\n|   mean| 4.649999999999999|\n| stddev|0.5175191657866612|\n|    min|               3.8|\n|    max|               5.4|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “NUMBER OF NULL RECORDS IN YEAR COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN MONTH COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN UNEMPLOYMENTRATE COLUMN IS:  0”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED”\n+——+—–+\n|  Year|count|\n+——+—–+\n|2020.0|   12|\n|2021.0|   12|\n+——+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|  8.0|    2|\n|  7.0|    2|\n|  1.0|    2|\n|  4.0|    2|\n| 11.0|    2|\n|  3.0|    2|\n|  2.0|    2|\n| 10.0|    2|\n|  6.0|    2|\n|  5.0|    2|\n|  9.0|    2|\n| 12.0|    2|\n+—–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|  UnemploymentRate|\n+——-+——————+\n|  count|                24|\n|   mean| 4.649999999999999|\n| stddev|0.5175191657866612|\n|    min|               3.8|\n|    max|               5.4|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DUPLICATE RECORDS”\n[1] “TOTAL RECORDS IN UNEMPLOYMENT DATAFRAME: 24”\n[1] “DUPLICATE RECORDS IN UNEMPLOYMENT DATAFRAME: 0”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER DUPLICATES REMOVAL - ROWS: 24 COLUMNS: 3”\n[1] “——————————————————————————–”\n[1] “FINAL UNEMPLOYMENT TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: 24 COLUMNS: 3”\n[1] “——————————————————————————–”\n[1] “TRANSFORMATION OF UNEMPLOYMENT DATAFRAME IS COMPLETED”\n\n\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=61","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=62","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=63","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=64","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=65","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=66","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=67","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=68","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=69","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=70","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=71","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=72","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=73","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=74","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=75","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=76","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=77","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=78","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=79","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=80","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=81","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=82","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=83","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=84","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=85","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=86","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=87","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=88","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=89","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=90","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=91","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=92","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=93"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1649018799847_-1339094586","id":"20220403-204639_1996318201","dateCreated":"2022-04-03T20:46:39+0000","dateStarted":"2022-04-19T12:24:08+0000","dateFinished":"2022-04-19T12:24:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11542"},{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 3: TRANSFORMATION STAGE - UK CRIME STATISTICS DATAFRAME\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#DATA VALIDITY - RECORDS BEFORE APPLYING TRANSFORMATIONS\npaste0('RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: ',nrow(ukc),' COLUMNS: ',ncol(ukc))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: EXTRACT NEW COLUMNS\n#NO ACTION NEEDED\n\n#DATA TRANSFORMATION: REMOVE ORIGINAL COLUMNS WHICH ARE NOT NEEDED\n#NO ACTION NEEDED\n\n#DATA VALIDITY - CHECK DATA TYPE OF EACH COLUMN\nprint('DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN')\nstr(ukc)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: CONVERT COLUMNS DATA TYPES\n#NO ACTION NEEDED\n\n#DATA TRANSFORMATION: APPLY FILTER\n#NO ACTION NEEDED\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA FOR INCONSISTENCY\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY')\nshowDF(count(groupBy(ukc, \"Country\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ukc, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ukc, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(ukc, 'NoofCrimes'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK NULL VALUES IN EACH COLUMN IN DATA FRAME\npaste(\"NUMBER OF NULL RECORDS IN COUNTRY COLUMN IS: \",nrow(SparkR::filter(ukc, isNull(ukc$Country))))\npaste(\"NUMBER OF NULL RECORDS IN YEAR COLUMN IS: \",nrow(SparkR::filter(ukc, isNull(ukc$Year))))\npaste(\"NUMBER OF NULL RECORDS IN MONTH COLUMN IS: \",nrow(SparkR::filter(ukc, isNull(ukc$Month))))\npaste(\"NUMBER OF NULL RECORDS IN NOOFCRIMES COLUMN IS: \",nrow(SparkR::filter(ukc, isNull(ukc$NoofCrimes))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: NULL VALUE TREATMENT\nukc$Country = ifelse(isNull(ukc$Country)==TRUE, 'Undefined', ukc$Country)\nukc$Year = ifelse(isNull(ukc$Year)==TRUE, -99, ukc$Year)\nukc$Month = ifelse(isNull(ukc$Month)==TRUE, -99, ukc$Month)\nukc$NoofCrimes = ifelse(isNull(ukc$NoofCrimes)==TRUE, -99, ukc$NoofCrimes)\n\n#DATA TRANSFORMATION: INVALID VALUE TREATMENT\nukc$Country=regexp_replace(ukc$Country,'NA',\"Undefined\")\n\n#DATA TRANSFORMATION: BLANK/SPACE DETECTED --> BY FREQUENCY DISTRIBUTION\nukc$Country = ifelse(trim(ukc$Country)=='', 'Undefined', ukc$Country)\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA AFTER NULL/INVALID DATA TREATMENT\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED')\nshowDF(count(groupBy(ukc, \"Country\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ukc, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ukc, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(ukc, 'NoofCrimes'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK DUPLICATE RECORDS\nprint('DATA VALIDITY - CHECK DUPLICATE RECORDS')\npaste0(\"TOTAL RECORDS IN UK NATION CRIME DATAFRAME: \",nrow(ukc))\npaste0(\"DUPLICATE RECORDS IN UK NATION CRIME DATAFRAME: \",(nrow(ukc)-nrow(collect(distinct(ukc)))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: REMOVE DUPLICATE RECORDS\nukc=distinct(ukc)\n\n#DATA VALIDITY - RECORDS AFTER DUPLICATES REMOVAL\npaste0('RECORDS AFTER DUPLICATES REMOVAL - ROWS: ',nrow(ukc),' COLUMNS: ',ncol(ukc))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - FINAL UK NATION CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE\npaste0('FINAL UK NATION CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: ',nrow(ukc),' COLUMNS: ',ncol(ukc))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(ukc, \"ukc\")\n\nprint(\"TRANSFORMATION OF UK NATION CRIME DATAFRAME IS COMPLETED\")","user":"anonymous","dateUpdated":"2022-04-19T14:18:00+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[1] “RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: 78 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN”\n'SparkDataFrame': 4 variables:\n $ Country   : chr “England and Wales” “England and Wales” “England and Wales” “England and Wales” “England and Wales” “\n $ Year      : int 2020 2020 2020 2020 2020 2020\n $ Month     : int 1 2 3 4 5 6\n $ NoofCrimes: int 447406 424962 402174 314449 356678 387726\n[1] ”——————————————————————————–“\n[1] \"DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY”\n+—————–+—–+\n|          Country|count|\n+—————–+—–+\n|England and Wales|   26|\n|  Nothern Ireland|   26|\n|         Scotland|   26|\n+—————–+—–+\n[1] “——————————————————————————–”\n+—-+—–+\n|Year|count|\n+—-+—–+\n|2022|    6|\n|2020|   36|\n|2021|   36|\n+—-+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|   12|    6|\n|    1|    9|\n|    6|    6|\n|    3|    6|\n|    5|    6|\n|    9|    6|\n|    4|    6|\n|    8|    6|\n|    7|    6|\n|   10|    6|\n|   11|    6|\n|    2|    9|\n+—–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|        NoofCrimes|\n+——-+——————+\n|  count|                78|\n|   mean|155304.57692307694|\n| stddev| 185887.0403724506|\n|    min|             15979|\n|    max|            481306|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “NUMBER OF NULL RECORDS IN COUNTRY COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN YEAR COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN MONTH COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN NOOFCRIMES COLUMN IS:  0”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED”\n+—————–+—–+\n|          Country|count|\n+—————–+—–+\n|England and Wales|   26|\n|  Nothern Ireland|   26|\n|         Scotland|   26|\n+—————–+—–+\n[1] “——————————————————————————–”\n+——+—–+\n|  Year|count|\n+——+—–+\n|2022.0|    6|\n|2020.0|   36|\n|2021.0|   36|\n+——+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|  8.0|    6|\n|  7.0|    6|\n|  1.0|    9|\n|  4.0|    6|\n| 11.0|    6|\n|  3.0|    6|\n|  2.0|    9|\n| 10.0|    6|\n|  6.0|    6|\n|  5.0|    6|\n|  9.0|    6|\n| 12.0|    6|\n+—–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|        NoofCrimes|\n+——-+——————+\n|  count|                78|\n|   mean|155304.57692307694|\n| stddev| 185887.0403724506|\n|    min|           15979.0|\n|    max|          481306.0|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DUPLICATE RECORDS”\n[1] “TOTAL RECORDS IN UK NATION CRIME DATAFRAME: 78”\n[1] “DUPLICATE RECORDS IN UK NATION CRIME DATAFRAME: 0”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER DUPLICATES REMOVAL - ROWS: 78 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “FINAL UK NATION CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: 78 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “TRANSFORMATION OF UK NATION CRIME DATAFRAME IS COMPLETED”\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1649118530865_-1687629989","id":"20220405-002850_831058675","dateCreated":"2022-04-05T00:28:50+0000","dateStarted":"2022-04-19T12:26:44+0000","dateFinished":"2022-04-19T12:27:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11543","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=94","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=95","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=96","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=97","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=98","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=99","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=100","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=101","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=102","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=103","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=104","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=105","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=106","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=107","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=108","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=109","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=110","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=111","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=112","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=113","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=114","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=115","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=116","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=117","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=118","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=119","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=120","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=121","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=122","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=123","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=124","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=125","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=126","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=127","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=128","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=129","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=130","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=131","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=132","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=133","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=134","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=135","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=136"],"interpreterSettingId":"spark2"}}},{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 3: TRANSFORMATION STAGE - YORKSHIRE AND HUMBER STOP SEARCH DATAFRAME\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#DATA VALIDITY - RECORDS BEFORE APPLYING TRANSFORMATIONS\npaste0('RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: ',nrow(ssf),' COLUMNS: ',ncol(ssf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: EXTRACT DATE COLUMNS\n\n#USE DATE FUNCTION TO EXTRACT DATE COLUMNS\nssf$Year=year(ssf$Date)\nssf$Month=month(ssf$Date)\n\n#REMOVE ORIGINAL COLUMNS WHICH ARE NOT NEEDED\nssf$Date = NULL\nssf$Partofapolicingoperation =NULL\nssf$Policingoperation = NULL\nssf$Latitude = NULL\nssf$Longitude = NULL\nssf$Gender = NULL\nssf$Agerange = NULL\nssf$Legislation = NULL\nssf$Outcomelinkedtoobjectofsearch = NULL\nssf$Removalofmorethanjustouterclothing = NULL\nssf$Fallswithin = NULL\nssf$Officerdefinedethnicity = NULL\nssf$Objectofsearch = NULL\nssf$Type = NULL\n\n#DATA TRANSFORMATION: STANDARDIZE COLUMN VALUES\nssf$Selfdefinedethnicity =  ifelse(ssf$Selfdefinedethnicity=='Asian/Asian British - Any other Asian background', 'Any other Asian', \n                            ifelse(ssf$Selfdefinedethnicity=='Asian/Asian British - Bangladeshi', 'Bangladeshi',\n                            ifelse(ssf$Selfdefinedethnicity=='Asian/Asian British - Chinese', 'Chinese',\n                            ifelse(ssf$Selfdefinedethnicity=='Asian/Asian British - Indian', 'Indian',\n                            ifelse(ssf$Selfdefinedethnicity=='Asian/Asian British - Pakistani', 'Pakistani',\n                            ifelse(ssf$Selfdefinedethnicity=='Black/African/Caribbean/Black British - African', 'African',\n                            ifelse(ssf$Selfdefinedethnicity=='Black/African/Caribbean/Black British - Any other Black/African/Caribbean background', 'Any other African Caribbean',\n                            ifelse(ssf$Selfdefinedethnicity=='Black/African/Caribbean/Black British - Caribbean', 'Caribbean',\n                            ifelse(ssf$Selfdefinedethnicity=='Mixed/Multiple ethnic groups - Any other Mixed/Multiple ethnic background', 'Any other Mixed',\n                            ifelse(ssf$Selfdefinedethnicity=='Mixed/Multiple ethnic groups - White and Asian', 'White and Asian',\n                            ifelse(ssf$Selfdefinedethnicity=='Mixed/Multiple ethnic groups - White and Black African', 'White and Black African',\n                            ifelse(ssf$Selfdefinedethnicity=='Mixed/Multiple ethnic groups - White and Black Caribbean', 'White and Black Caribbean',\n                            ifelse(ssf$Selfdefinedethnicity=='Other ethnic group - Any other ethnic group', 'Any other ethnic group',\n                            ifelse(ssf$Selfdefinedethnicity=='Other ethnic group - Arab', 'Arab',\n                            ifelse(ssf$Selfdefinedethnicity=='Other ethnic group - Not stated', 'Other ethnic group Not stated',\n                            ifelse(ssf$Selfdefinedethnicity=='White - Any other White background', 'Any other White',\n                            ifelse(ssf$Selfdefinedethnicity=='White - English/Welsh/Scottish/Northern Irish/British', 'English/Welsh/Scottish/Northern Irish/British',\n                            ifelse(ssf$Selfdefinedethnicity=='White - Gypsy or Irish Traveller', 'Gypsy or Irish Traveller',\n                            ifelse(ssf$Selfdefinedethnicity=='White - Irish', 'Irish', 'Undefined')))))))))))))))))))\n\n\n#EXTRACT NUMBER OF STOP SEARCHES FROM GROUPING COLUMNS\n#A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns\n#the result as a SparkDataFrame.\ncreateOrReplaceTempView(ssf, \"ssf\")\n\n#SQL Query\nssf <- sql(\"select Year, Month, Selfdefinedethnicity, Outcome, count(*) as NoofStopsearch from ssf group by Year, Month, Selfdefinedethnicity, Outcome\")\n\n#DATA VALIDITY - CHECK DATA TYPE OF EACH COLUMN\nprint('DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN')\nstr(ssf)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: CONVERT COLUMNS DATA TYPES \n#NO ACTION NEEDED\n\n#DATA TRANSFORMATION: APPLY FILTER FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020\nssf=subset(ssf, ssf$Year >= 2020)\n\n#DATA VALIDITY - RECORDS AFTER APPLYING FILTER\npaste0('RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: ',nrow(ssf),' COLUMNS: ',ncol(ssf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA FOR INCONSISTENCY\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY')\nshowDF(count(groupBy(ssf, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Selfdefinedethnicity\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Outcome\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(ssf, 'NoofStopsearch'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK NULL VALUES IN EACH COLUMN IN DATA FRAME\npaste(\"NUMBER OF NULL RECORDS IN YEAR COLUMN IS: \",nrow(SparkR::filter(ssf, isNull(ssf$Year))))\npaste(\"NUMBER OF NULL RECORDS IN MONTH COLUMN IS: \",nrow(SparkR::filter(ssf, isNull(ssf$Month))))\npaste(\"NUMBER OF NULL RECORDS IN SELFDEFINEDETHNICITY COLUMN IS: \",nrow(SparkR::filter(ssf, isNull(ssf$Selfdefinedethnicity))))\npaste(\"NUMBER OF NULL RECORDS IN OUTCOME COLUMN IS: \",nrow(SparkR::filter(ssf, isNull(ssf$Outcome))))\npaste(\"NUMBER OF NULL RECORDS IN NOOFSTOPSEARCH COLUMN IS: \",nrow(SparkR::filter(ssf, isNull(ssf$NoofStopsearch))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: NULL VALUE TREATMENT\nssf$Year = ifelse(isNull(ssf$Year)==TRUE, -99, ssf$Year)\nssf$Month = ifelse(isNull(ssf$Month)==TRUE, -99, ssf$Month)\nssf$Selfdefinedethnicity = ifelse(isNull(ssf$Selfdefinedethnicity)==TRUE, 'Undefined', ssf$Selfdefinedethnicity)\nssf$Outcome = ifelse(isNull(ssf$Outcome)==TRUE, 'Undefined', ssf$Outcome)\nssf$NoofStopsearch = ifelse(isNull(ssf$NoofStopsearch)==TRUE, -99, ssf$NoofStopsearch)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: INVALID VALUE TREATMENT\nssf$Selfdefinedethnicity=regexp_replace(ssf$Selfdefinedethnicity,'NA',\"Undefined\")\nssf$Outcome=regexp_replace(ssf$Outcome,'NA',\"Undefined\")\n\n#DATA TRANSFORMATION: BLANK/SPACE DETECTED --> BY FREQUENCY DISTRIBUTION\nssf$Selfdefinedethnicity = ifelse(trim(ssf$Selfdefinedethnicity)=='', 'Undefined', ssf$Selfdefinedethnicity)\nssf$Outcome = ifelse(trim(ssf$Outcome)=='', 'Undefined', ssf$Outcome)\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA AFTER NULL/INVALID DATA TREATMENT\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED')\nshowDF(count(groupBy(ssf, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Selfdefinedethnicity\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ssf, \"Outcome\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(describe(ssf, 'NoofStopsearch'))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK DUPLICATE RECORDS\nprint('DATA VALIDITY - CHECK DUPLICATE RECORDS')\npaste0(\"TOTAL RECORDS IN STOP SEARCH DATAFRAME: \",nrow(ssf))\npaste0(\"DUPLICATE RECORDS IN STOP SEARCH DATAFRAME: \",(nrow(ssf)-nrow(collect(distinct(ssf)))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: REMOVE DUPLICATE RECORDS\nssf=distinct(ssf)\n\n#DATA VALIDITY - RECORDS AFTER DUPLICATES REMOVAL\npaste0('RECORDS AFTER DUPLICATES REMOVAL - ROWS: ',nrow(ssf),' COLUMNS: ',ncol(ssf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - FINAL STOP SEARCH TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE\npaste0('FINAL STOP SEARCH TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: ',nrow(ssf),' COLUMNS: ',ncol(ssf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(ssf, \"ssf\")\n\nprint(\"TRANSFORMATION OF STOP SEARCH DATAFRAME IS COMPLETED\")","user":"anonymous","dateUpdated":"2022-04-19T14:19:06+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[1] “RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: 96163 COLUMNS: 16”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN”\n'SparkDataFrame': 5 variables:\n $ Year                : int 2021 2021 2021 2021 2021 2022\n $ Month               : int 3 7 7 8 10 2\n $ Selfdefinedethnicity: chr “Other ethnic group Not stated” “White and Asian” “Irish” “Other ethnic group Not stated” “\n $ Outcome             : chr \"Khat or Cannabis warning” “A no further action disposal” “Summons / charged by post” “Arre\n $ NoofStopsearch      : num 11 13 2 51 7 5\n[1] ”——————————————————————————–“\n[1] \"RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: 2598 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY”\n+—-+—–+\n|Year|count|\n+—-+—–+\n|2022|  183|\n|2020| 1267|\n|2021| 1148|\n+—-+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|   12|  206|\n|    1|  302|\n|    6|  193|\n|    3|  205|\n|    5|  207|\n|    9|  192|\n|    4|  206|\n|    8|  194|\n|    7|  192|\n|   10|  204|\n|   11|  203|\n|    2|  294|\n+—–+—–+\n[1] “——————————————————————————–”\n+——————–+—–+\n|Selfdefinedethnicity|count|\n+——————–+—–+\n|     White and Asian|  114|\n|Any other African…|  142|\n|              Indian|  102|\n|Other ethnic grou…|  196|\n|             Chinese|    6|\n|                Arab|    1|\n|             African|  154|\n|               Irish|   94|\n|Any other ethnic …|  122|\n|     Any other White|  170|\n|English/Welsh/Sco…|  207|\n|White and Black A…|   73|\n|           Pakistani|  191|\n|           Caribbean|  137|\n|         Bangladeshi|  118|\n|Gypsy or Irish Tr…|  130|\n|           Undefined|  201|\n|     Any other Asian|  180|\n|White and Black C…|  136|\n|     Any other Mixed|  124|\n+——————–+—–+\n[1] “——————————————————————————–”\n+——————–+—–+\n|             Outcome|count|\n+——————–+—–+\n|Community resolution|  420|\n|                null|  343|\n|Penalty Notice fo…|  140|\n|Caution (simple o…|  193|\n|Khat or Cannabis …|  253|\n|              Arrest|  452|\n|A no further acti…|  470|\n|Summons / charged…|  327|\n+——————–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|    NoofStopsearch|\n+——-+——————+\n|  count|              2598|\n|   mean|37.014241724403384|\n| stddev|135.57314901679635|\n|    min|                 1|\n|    max|              1648|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “NUMBER OF NULL RECORDS IN YEAR COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN MONTH COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN SELFDEFINEDETHNICITY COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN OUTCOME COLUMN IS:  343”\n[1] “NUMBER OF NULL RECORDS IN NOOFSTOPSEARCH COLUMN IS:  0”\n[1] “——————————————————————————–”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED”\n+——+—–+\n|  Year|count|\n+——+—–+\n|2022.0|  183|\n|2020.0| 1267|\n|2021.0| 1148|\n+——+—–+\n[1] “——————————————————————————–”\n+—–+—–+\n|Month|count|\n+—–+—–+\n|  8.0|  194|\n|  7.0|  192|\n|  1.0|  302|\n|  4.0|  206|\n| 11.0|  203|\n|  3.0|  205|\n|  2.0|  294|\n| 10.0|  204|\n|  6.0|  193|\n|  5.0|  207|\n|  9.0|  192|\n| 12.0|  206|\n+—–+—–+\n[1] “——————————————————————————–”\n+——————–+—–+\n|Selfdefinedethnicity|count|\n+——————–+—–+\n|     White and Asian|  114|\n|              Indian|  102|\n|Any other African…|  142|\n|Other ethnic grou…|  196|\n|             Chinese|    6|\n|                Arab|    1|\n|             African|  154|\n|               Irish|   94|\n|Any other ethnic …|  122|\n|     Any other White|  170|\n|English/Welsh/Sco…|  207|\n|White and Black A…|   73|\n|           Pakistani|  191|\n|           Caribbean|  137|\n|         Bangladeshi|  118|\n|Gypsy or Irish Tr…|  130|\n|           Undefined|  201|\n|     Any other Asian|  180|\n|White and Black C…|  136|\n|     Any other Mixed|  124|\n+——————–+—–+\n[1] “——————————————————————————–”\n+——————–+—–+\n|             Outcome|count|\n+——————–+—–+\n|Community resolution|  420|\n|Penalty Notice fo…|  140|\n|Caution (simple o…|  193|\n|Khat or Cannabis …|  253|\n|              Arrest|  452|\n|           Undefined|  343|\n|A no further acti…|  470|\n|Summons / charged…|  327|\n+——————–+—–+\n[1] “——————————————————————————–”\n+——-+——————+\n|summary|    NoofStopsearch|\n+——-+——————+\n|  count|              2598|\n|   mean|37.014241724403384|\n| stddev|135.57314901679632|\n|    min|               1.0|\n|    max|            1648.0|\n+——-+——————+\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DUPLICATE RECORDS”\n[1] “TOTAL RECORDS IN STOP SEARCH DATAFRAME: 2598”\n[1] “DUPLICATE RECORDS IN STOP SEARCH DATAFRAME: 0”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER DUPLICATES REMOVAL - ROWS: 2598 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “FINAL STOP SEARCH TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: 2598 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “TRANSFORMATION OF STOP SEARCH DATAFRAME IS COMPLETED”\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1649119552209_645070875","id":"20220405-004552_1761770451","dateCreated":"2022-04-05T00:45:52+0000","dateStarted":"2022-04-19T13:02:37+0000","dateFinished":"2022-04-19T13:04:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11544","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=137","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=138","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=139","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=140","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=141","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=142","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=143","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=144","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=145","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=146","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=147","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=148","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=149","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=150","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=151","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=152","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=153","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=154","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=155","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=156","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=157","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=158","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=159","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=160","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=161","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=162","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=163","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=164","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=165","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=166","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=167","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=168","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=169","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=170","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=171","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=172","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=173","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=174","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=175","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=176","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=177","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=178","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=179","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=180","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=181","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=182","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=183","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=184","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=185","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=186","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=187","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=188","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=189","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=190","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=191","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=192","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=193","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=194","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=195"],"interpreterSettingId":"spark2"}}},{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 3: TRANSFORMATION STAGE - YORKSHIRE AND HUMBER OUTCOME DATAFRAME\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#DATA VALIDITY - RECORDS BEFORE APPLYING TRANSFORMATIONS\npaste0('RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: ',nrow(oc),' COLUMNS: ',ncol(oc))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: EXTRACT DATE COLUMNS USING SUBSTRING FUNCTION\noc$Year = substr(oc$Month, 1, 4)\noc$Month = substr(oc$Month, 6, 7)\n\n#REMOVE ORIGINAL COLUMNS WHICH ARE NOT NEEDED\noc$Reportedby = NULL\noc$Fallswithin = NULL\noc$Longitude = NULL\noc$Latitude = NULL\noc$Location = NULL\noc$LSOAcode = NULL\noc$LSOAname = NULL\n\n#DATA VALIDITY - CHECK DATA TYPE OF EACH COLUMN\nprint('DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN')\nstr(oc)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: CONVERT COLUMNS DATA TYPES\noc$Year <- SparkR::cast(oc$Year, \"double\")\noc$Month <- SparkR::cast(oc$Month, \"double\")\n\n#DATA TRANSFORMATION: APPLY FILTER FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020\noc=subset(oc, oc$Year >= 2020)\n\n#DATA VALIDITY - RECORDS AFTER APPLYING FILTER\npaste0('RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: ',nrow(oc),' COLUMNS: ',ncol(oc))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA FOR INCONSISTENCY\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY')\nshowDF(count(groupBy(oc, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(oc, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(oc, \"Outcometype\")))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK NULL VALUES IN EACH COLUMN IN DATA FRAME\npaste(\"NUMBER OF NULL RECORDS IN CRIMEID COLUMN IS: \",nrow(SparkR::filter(oc, isNull(oc$CrimeID))))\npaste(\"NUMBER OF NULL RECORDS IN YEAR COLUMN IS: \",nrow(SparkR::filter(oc, isNull(oc$Year))))\npaste(\"NUMBER OF NULL RECORDS IN MONTH COLUMN IS: \",nrow(SparkR::filter(oc, isNull(oc$Month))))\npaste(\"NUMBER OF NULL RECORDS IN OUTCOMETYPE COLUMN IS: \",nrow(SparkR::filter(oc, isNull(oc$Outcometype))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: NULL VALUE TREATMENT\noc$Year = ifelse(isNull(oc$Year)==TRUE, -99, oc$Year)\noc$Month = ifelse(isNull(oc$Month)==TRUE, -99, oc$Month)\noc$Outcometype = ifelse(isNull(oc$Outcometype)==TRUE, 'Undefined', oc$Outcometype)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: INVALID VALUE TREATMENT\noc$Outcometype=regexp_replace(oc$Outcometype,'NA',\"Undefined\")\n\n#DATA TRANSFORMATION: BLANK/SPACE DETECTED --> BY FREQUENCY DISTRIBUTION\noc$Outcometype = ifelse(trim(oc$Outcometype)=='', 'Undefined', oc$Outcometype)\n\n#DATA TRANSFORMATION: DELETE CRIMEID - ID VARIABLE, IF NULL IS PRESENT\noc=dropna(oc, how = \"any\")\n\n#DATA TRANSFORMATION: DELETE CRIMEID VARIABLE LESS THAN STANDARD LENGTH 64\noc$flag = ifelse(length(oc$CrimeID) < 64, 'True', 'False')\nprint('DATA VALIDITY - FREQUENCY DISTRIBUTION - CRIMEID LESSTHAN 64 CHARACTERS')\nshowDF(count(groupBy(oc, \"flag\")))\noc=subset(oc, oc$flag == 'False')\noc$flag = NULL\n\n#DATA TRANSFORMATION: REMOVE RECORDS WITH MORETHAN SINGLE OUTCOME IN SAME YEAR MONTH\n#A SparkDataFrame can also be registered as a temporary view in Spark SQL and that allows you to run SQL queries over its data. The sql function enables applications to run SQL queries programmatically and returns\n#the result as a SparkDataFrame.\ncreateOrReplaceTempView(oc, \"oc\")\n\n#SQL Query\noc1 <- sql(\"select CrimeID, Max(Year) as MaxYear, Max(Month) as MaxMonth from oc group by  CrimeID\")\n\ncreateOrReplaceTempView(oc1, \"oc1\")\n\nocfinal <- sql(\"select a.* from oc a inner join oc1 b on a.CrimeID=b.CrimeID and a.Year=b.MaxYear and a.Month=b.MaxMonth\")\n\nrm(oc, oc1)\n\n#Certain CrimeID's have different outcomes for same Year and Month. Hence, CrimeID which 1st occurence is taken for analysis\nocfinal=dropDuplicates(ocfinal, \"CrimeID\")\n\n#DATA VALIDITY - REMOVE NULL ID RECORDS \npaste0('RECORDS AFTER INVALID CRIMEID REMOVAL AND RECENT OUTCOME PER CRIMEID - ROWS: ',nrow(ocfinal),' COLUMNS: ',ncol(ocfinal))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA AFTER NULL/INVALID DATA TREATMENT\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED')\nshowDF(count(groupBy(ocfinal, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ocfinal, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(ocfinal, \"Outcometype\")))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK DUPLICATE RECORDS\nprint('DATA VALIDITY - CHECK DUPLICATE RECORDS')\npaste0(\"TOTAL RECORDS IN OUTCOME DATAFRAME: \",nrow(ocfinal))\nnodup=distinct(ocfinal)\npaste0(\"DUPLICATE RECORDS IN OUTCOME DATAFRAME: \",(nrow(ocfinal)-nrow(nodup)))\nprint(\"--------------------------------------------------------------------------------\")\nrm(nodup)\n\n#DATA TRANSFORMATION: REMOVE DUPLICATE RECORDS\nssf=distinct(ocfinal)\n\n#DATA VALIDITY - RECORDS AFTER DUPLICATES REMOVAL\npaste0('RECORDS AFTER DUPLICATES REMOVAL - ROWS: ',nrow(ocfinal),' COLUMNS: ',ncol(ocfinal))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - FINAL STOP SEARCH TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE\npaste0('FINAL OUTCOME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: ',nrow(ocfinal),' COLUMNS: ',ncol(ocfinal))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(ocfinal, \"ocfinal\")\n\nprint(\"TRANSFORMATION OF OUTCOME DATAFRAME IS COMPLETED\")\n\n#Scenario-1: Outcome Duplicate Sample\n#-------------------------------------\n# CrimeID\n# e9ff6a15dec4ec79aa13dca06a5a1afa0beed909b605a428788eda1a9d23c184\n# 2020-08\n# West Yorkshire Police\n\n# Scenario-2: Outcome Type Duplicates:\n#-------------------------------------\n# 0257496e011c0525f40f9e1a908475917a4d5ade87523db62709a51174628631\n# outcome changes by month year for same person. Hence, most recent should be retrieved.\n\n# Scenario-3: Outcome Type Duplicates - Single Person has different outcome in same year and month:\n#-------------------------------------\n# a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4\n","user":"anonymous","dateUpdated":"2022-04-19T15:19:09+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[1] “RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: 1262104 COLUMNS: 10”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN”\n'SparkDataFrame': 4 variables:\n $ CrimeID    : chr “8756dda399f9753e979ba6c754f099b68ea12900da7798d177c9a4ab19b4c373” “c1b92d172bc966f2e90f9af73775e166\n $ Month      : chr \"01” “01” “01” “01” “01” “01”\n $ Outcometype: chr “Suspect charged” “Unable to prosecute suspect” “Unable to prosecute suspect” “Unable to prosecute s\n $ Year       : chr \"2020” “2020” “2020” “2020” “2020” “2020”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: 1262104 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY”\n+——+——+\n|  Year| count|\n+——+——+\n|2022.0| 99456|\n|2020.0|576338|\n|2021.0|586310|\n+——+——+\n[1] “——————————————————————————–”\n+—–+——+\n|Month| count|\n+—–+——+\n|  8.0| 96468|\n|  7.0|104515|\n|  1.0|148093|\n|  4.0| 95092|\n| 11.0|111715|\n|  3.0| 91262|\n|  2.0|131628|\n| 10.0|105515|\n|  6.0| 93777|\n|  5.0| 93288|\n|  9.0| 98074|\n| 12.0| 92677|\n+—–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n|         Outcometype| count|\n+——————–+——+\n|     Suspect charged|125152|\n|Offender given pe…|   931|\n|Suspect charged a…|   883|\n|    Local resolution| 40916|\n|Offender given a …| 17236|\n|Investigation com…|438007|\n|Further investiga…| 13862|\n|Further action is…| 12713|\n|Action to be take…| 14125|\n|Offender given a …|  3270|\n|Formal action is …|  9311|\n|Unable to prosecu…|585698|\n+——————–+——+\n[1] “——————————————————————————–”\n[1] “NUMBER OF NULL RECORDS IN CRIMEID COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN YEAR COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN MONTH COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN OUTCOMETYPE COLUMN IS:  0”\n[1] “——————————————————————————–”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - FREQUENCY DISTRIBUTION - CRIMEID LESSTHAN 64 CHARACTERS”\n+—–+——-+\n| flag|  count|\n+—–+——-+\n|False|1262104|\n+—–+——-+\n[1] “RECORDS AFTER INVALID CRIMEID REMOVAL AND RECENT OUTCOME PER CRIMEID - ROWS: 1131460 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED”\n+——+——+\n|  Year| count|\n+——+——+\n|2022.0| 90332|\n|2020.0|514732|\n|2021.0|526396|\n+——+——+\n[1] “——————————————————————————–”\n+—–+——+\n|Month| count|\n+—–+——+\n|  8.0| 87608|\n|  7.0| 93479|\n|  1.0|131026|\n|  4.0| 84269|\n| 11.0| 96106|\n|  3.0| 83556|\n|  2.0|119539|\n| 10.0| 95045|\n|  6.0| 84254|\n|  5.0| 83395|\n|  9.0| 89152|\n| 12.0| 84031|\n+—–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n|         Outcometype| count|\n+——————–+——+\n|     Suspect charged| 93141|\n|Offender given pe…|   735|\n|Suspect charged a…|   680|\n|    Local resolution| 36416|\n|Offender given a …| 14491|\n|Investigation com…|428765|\n|Further investiga…| 12328|\n|Further action is…| 11919|\n|Action to be take…| 12959|\n|Offender given a …|  2783|\n|Formal action is …|  6413|\n|Unable to prosecu…|510830|\n+——————–+——+\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DUPLICATE RECORDS”\n[1] “TOTAL RECORDS IN OUTCOME DATAFRAME: 1131460”\n[1] “DUPLICATE RECORDS IN OUTCOME DATAFRAME: 0”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER DUPLICATES REMOVAL - ROWS: 1131460 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “FINAL OUTCOME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: 1131460 COLUMNS: 4”\n[1] “——————————————————————————–”\n[1] “TRANSFORMATION OF OUTCOME DATAFRAME IS COMPLETED”\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1649124142960_-764491141","id":"20220405-020222_1051417958","dateCreated":"2022-04-05T02:02:22+0000","dateStarted":"2022-04-19T15:02:57+0000","dateFinished":"2022-04-19T15:16:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11545","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=293","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=294","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=295","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=296","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=297","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=298","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=299","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=300","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=301","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=302","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=303","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=304","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=305","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=306","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=307","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=308","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=309","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=310","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=311","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=312","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=313","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=314","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=315","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=316","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=317","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=318","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=319","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=320","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=321","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=322","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=323","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=324","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=325","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=326","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=327","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=328","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=329","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=330","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=331","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=332","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=333","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=334","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=335","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=336","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=337","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=338","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=339","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=340"],"interpreterSettingId":"spark2"}}},{"text":"%spark2.r\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#**********************************\tAuthor: ADMP Group 6\t******************************************\n#**********************************\tDate  : 14 Apr 2022\t    ******************************************\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#Section 3: TRANSFORMATION STAGE - YORKSHIRE AND HUMBER STREET CRIME DATAFRAME\n#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n#DATA VALIDITY - RECORDS BEFORE APPLYING TRANSFORMATIONS\npaste0('RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: ',nrow(scf),' COLUMNS: ',ncol(scf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: EXTRACT DATE COLUMNS USING SUBSTRING FUNCTION\nscf$Year = substr(scf$Month, 1, 4)\nscf$Month = substr(scf$Month, 6, 7)\n\n#REMOVE ORIGINAL COLUMNS WHICH ARE NOT NEEDED\nscf$Reportedby = NULL\nscf$Fallswithin = NULL\nscf$Location = NULL\nscf$Context = NULL\nscf$Latitude = NULL\nscf$Longitude = NULL\nscf$LSOAcode = NULL\nscf$LSOAname = NULL\n\n#DATA VALIDITY - CHECK DATA TYPE OF EACH COLUMN\nprint('DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN')\nstr(scf)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: CONVERT COLUMNS DATA TYPES\nscf$Year <- SparkR::cast(scf$Year, \"double\")\nscf$Month <- SparkR::cast(scf$Month, \"double\")\n\n#DATA TRANSFORMATION: APPLY FILTER FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020\nscf=subset(scf, scf$Year >= 2020)\n\n#DATA VALIDITY - RECORDS AFTER APPLYING FILTER\npaste0('RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: ',nrow(scf),' COLUMNS: ',ncol(scf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA FOR INCONSISTENCY\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY')\nshowDF(count(groupBy(scf, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Crimetype\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Lastoutcomecategory\")))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK NULL VALUES IN EACH COLUMN IN DATA FRAME\npaste(\"NUMBER OF NULL RECORDS IN CRIMEID COLUMN IS: \",nrow(SparkR::filter(scf, isNull(scf$CrimeID))))\npaste(\"NUMBER OF NULL RECORDS IN YEAR COLUMN IS: \",nrow(SparkR::filter(scf, isNull(scf$Year))))\npaste(\"NUMBER OF NULL RECORDS IN MONTH COLUMN IS: \",nrow(SparkR::filter(scf, isNull(scf$Month))))\npaste(\"NUMBER OF NULL RECORDS IN CRIMETYPE COLUMN IS: \",nrow(SparkR::filter(scf, isNull(scf$Crimetype))))\npaste(\"NUMBER OF NULL RECORDS IN LASTOUTCOMECATEGORY COLUMN IS: \",nrow(SparkR::filter(scf, isNull(scf$Lastoutcomecategory))))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: NULL VALUE TREATMENT\nscf$Year = ifelse(isNull(scf$Year)==TRUE, -99, scf$Year)\nscf$Month = ifelse(isNull(scf$Month)==TRUE, -99, scf$Month)\nscf$Crimetype = ifelse(isNull(scf$Crimetype)==TRUE, 'Undefined', scf$Crimetype)\nscf$Lastoutcomecategory = ifelse(isNull(scf$Lastoutcomecategory)==TRUE, 'Undefined', scf$Lastoutcomecategory)\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: INVALID VALUE TREATMENT\nscf$Crimetype=regexp_replace(scf$Crimetype,'NA',\"Undefined\")\nscf$Lastoutcomecategory=regexp_replace(scf$Lastoutcomecategory,'NA',\"Undefined\")\n\n#DATA TRANSFORMATION: BLANK/SPACE DETECTED --> BY FREQUENCY DISTRIBUTION\nscf$Crimetype = ifelse(trim(scf$Crimetype)=='', 'Undefined', scf$Crimetype)\nscf$Lastoutcomecategory = ifelse(trim(scf$Lastoutcomecategory)=='', 'Undefined', scf$Lastoutcomecategory)\n\n#DATA TRANSFORMATION: DELETE CRIMEID - ID VARIABLE, IF NULL IS PRESENT\nscf=dropna(scf, how = \"any\")\n\n#DATA TRANSFORMATION: DELETE CRIMEID VARIABLE LESS THAN STANDARD LENGTH 64\nscf$flag = ifelse(length(scf$CrimeID) < 64, 'True', 'False')\nprint('DATA VALIDITY - FREQUENCY DISTRIBUTION - CRIMEID LESSTHAN 64 CHARACTERS')\nshowDF(count(groupBy(scf, \"flag\")))\nscf=subset(scf, scf$flag == 'False')\nscf$flag = NULL\n\n#DATA VALIDITY - REMOVE NULL ID RECORDS \npaste0('RECORDS AFTER INVALID CRIMEID REMOVAL - ROWS: ',nrow(scf),' COLUMNS: ',ncol(scf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR NUMERICAL AND CATEGORICAL DATA AFTER NULL/INVALID DATA TREATMENT\nprint('DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED')\nshowDF(count(groupBy(scf, \"Year\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Month\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Crimetype\")))\nprint(\"--------------------------------------------------------------------------------\")\nshowDF(count(groupBy(scf, \"Lastoutcomecategory\")))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - CHECK DUPLICATE RECORDS\nprint('DATA VALIDITY - CHECK DUPLICATE RECORDS')\npaste0(\"TOTAL RECORDS IN OUTCOME DATAFRAME: \",nrow(scf))\nnodup=distinct(scf)\npaste0(\"DUPLICATE RECORDS IN OUTCOME DATAFRAME: \",(nrow(scf)-nrow(scf)))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA TRANSFORMATION: REMOVE DUPLICATE RECORDS\nscf=distinct(scf)\n\n#DATA VALIDITY - RECORDS AFTER DUPLICATES REMOVAL\npaste0('RECORDS AFTER DUPLICATES REMOVAL - ROWS: ',nrow(scf),' COLUMNS: ',ncol(scf))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(scf, \"scf\")\n\n#DATA TRANSFORMATION - GET LATEST OUTCOME TO STREET CRIME DATASET VIA LEFT JOIN\nstreetfinal=sql(\"select a.*, b.Outcometype from scf a left join ocfinal b on a.CrimeID=b.CrimeID\")\n\n#DATA TRANSFORMATION - CREATE OUTCOME COLUMN WHICH HAS RECENT OUTCOMES EITHER FROM OUTCOME DATAFRAME OR STREET CRIME DATAFRAME\nstreetfinal$outcome = ifelse(isNull(streetfinal$Outcometype)==TRUE, streetfinal$Lastoutcomecategory, streetfinal$Outcometype)\n\n#DATA TRANSFORMATION - REMOVE UNWANTED COLUMNS\nstreetfinal$Outcometype=NULL\nstreetfinal$Lastoutcomecategory=NULL\n\n#DATA VALIDITY - FINAL STREET CRIME TABLE ROWS AND COLUMNS AFTER GETTING UPDATED OUTCOMES\npaste0('FINAL STREET CRIME TABLE ROWS AND COLUMNS AFTER JOINING OUTCOME DATASET - ROWS: ',nrow(streetfinal),' COLUMNS: ',ncol(streetfinal))\nprint(\"--------------------------------------------------------------------------------\")\n\n#DATA VALIDITY - FINAL STREET CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE\npaste0('FINAL STREET CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: ',nrow(streetfinal),' COLUMNS: ',ncol(streetfinal))\nprint(\"--------------------------------------------------------------------------------\")\n\n#REGISTER SPARK DATAFRAME AS TEMP DATAFRAME - TO CREATE FACT DIMENSION SCHEMA CREATION\ncreateOrReplaceTempView(streetfinal, \"streetfinal\")\n\nrm(scf, ocfinal)\nprint(\"TRANSFORMATION OF STREET CRIME DATAFRAME IS COMPLETED\")\n","user":"anonymous","dateUpdated":"2022-04-19T15:40:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n[1] “RECORDS BEFORE APPLYING TRANSFORMATIONS - ROWS: 1429456 COLUMNS: 12”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DATATYPES OF EACH COLUMN”\n'SparkDataFrame': 5 variables:\n $ CrimeID            : chr “b7fd5c3d21d84819bf81644db4054bc72e49e9951a26d8182fa880c9f3feb690” “faed29321bc835ca7db802a2\n $ Month              : chr \"01” “01” “01” “01” “01” “01”\n $ Crimetype          : chr “Burglary” “Public order” “Criminal damage and arson” “Violence and sexual offences” “Violen\n $ Lastoutcomecategory: chr \"Investigation complete; no suspect identified” “Unable to prosecute suspect” “Unable to pro\n $ Year               : chr \"2020” “2020” “2020” “2020” “2020” “2020”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER FILTERING FOR ANALYSIS PERIOD GREATER THAN OR EQUAL TO 2020 - ROWS: 1429456 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION FOR INCONSISTENCY”\n+——+——+\n|  Year| count|\n+——+——+\n|2022.0|105072|\n|2020.0|660013|\n|2021.0|664371|\n+——+——+\n[1] “——————————————————————————–”\n+—–+——+\n|Month| count|\n+—–+——+\n|  8.0|118214|\n|  7.0|121532|\n|  1.0|158095|\n|  4.0|103074|\n| 11.0|110469|\n|  3.0|110628|\n|  2.0|149767|\n| 10.0|117177|\n|  6.0|114591|\n|  5.0|111126|\n|  9.0|113878|\n| 12.0|100905|\n+—–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n|           Crimetype| count|\n+——————–+——+\n|       Bicycle theft| 10847|\n|        Public order|144391|\n|               Drugs| 35774|\n|         Other crime| 33463|\n|             Robbery| 10524|\n|Criminal damage a…|129634|\n|Theft from the pe…|  8154|\n|         Shoplifting| 62380|\n|            Burglary| 70702|\n|         Other theft| 80703|\n|Possession of wea…| 10503|\n|Violence and sexu…|507418|\n|       Vehicle crime| 66056|\n|Anti-social behav…|258907|\n+——————–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n| Lastoutcomecategory| count|\n+——————–+——+\n|Court result unav…| 58361|\n|Offender given pe…|   697|\n|Suspect charged a…|   584|\n|                null|258907|\n|    Local resolution| 34191|\n|Offender given a …| 13117|\n|Investigation com…|416068|\n| Under investigation| 54356|\n|Awaiting court ou…| 23819|\n|Further investiga…| 11373|\n|Further action is…| 11927|\n|Action to be take…| 12513|\n|Offender given a …|  2677|\n|Formal action is …|  4800|\n|Status update una…| 48484|\n|Unable to prosecu…|477582|\n+——————–+——+\n[1] “——————————————————————————–”\n[1] “NUMBER OF NULL RECORDS IN CRIMEID COLUMN IS:  258907”\n[1] “NUMBER OF NULL RECORDS IN YEAR COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN MONTH COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN CRIMETYPE COLUMN IS:  0”\n[1] “NUMBER OF NULL RECORDS IN LASTOUTCOMECATEGORY COLUMN IS:  258907”\n[1] “——————————————————————————–”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - FREQUENCY DISTRIBUTION - CRIMEID LESSTHAN 64 CHARACTERS”\n+—–+——-+\n| flag|  count|\n+—–+——-+\n|False|1170549|\n+—–+——-+\n[1] “RECORDS AFTER INVALID CRIMEID REMOVAL - ROWS: 1170549 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK FREQUENCY DISTRIBUTION TO VALIDATE NULL/INVALID DATA TREATMENT PERFORMED”\n+——+——+\n|  Year| count|\n+——+——+\n|2022.0| 93623|\n|2020.0|523392|\n|2021.0|553534|\n+——+——+\n[1] “——————————————————————————–”\n+—–+——+\n|Month| count|\n+—–+——+\n|  8.0| 95223|\n|  7.0| 97737|\n|  1.0|134448|\n|  4.0| 80056|\n| 11.0| 92916|\n|  3.0| 88617|\n|  2.0|126894|\n| 10.0| 96786|\n|  6.0| 90472|\n|  5.0| 86789|\n|  9.0| 94290|\n| 12.0| 86321|\n+—–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n|           Crimetype| count|\n+——————–+——+\n|       Bicycle theft| 10847|\n|        Public order|144391|\n|               Drugs| 35774|\n|         Other crime| 33463|\n|             Robbery| 10524|\n|Criminal damage a…|129634|\n|Theft from the pe…|  8154|\n|         Shoplifting| 62380|\n|            Burglary| 70702|\n|         Other theft| 80703|\n|Possession of wea…| 10503|\n|Violence and sexu…|507418|\n|       Vehicle crime| 66056|\n+——————–+——+\n[1] “——————————————————————————–”\n+——————–+——+\n| Lastoutcomecategory| count|\n+——————–+——+\n|Court result unav…| 58361|\n|Offender given pe…|   697|\n|Suspect charged a…|   584|\n|    Local resolution| 34191|\n|Offender given a …| 13117|\n|Investigation com…|416068|\n| Under investigation| 54356|\n|Awaiting court ou…| 23819|\n|Further investiga…| 11373|\n|Further action is…| 11927|\n|Action to be take…| 12513|\n|Offender given a …|  2677|\n|Formal action is …|  4800|\n|Status update una…| 48484|\n|Unable to prosecu…|477582|\n+——————–+——+\n[1] “——————————————————————————–”\n[1] “DATA VALIDITY - CHECK DUPLICATE RECORDS”\n[1] “TOTAL RECORDS IN OUTCOME DATAFRAME: 1170549”\n[1] “DUPLICATE RECORDS IN OUTCOME DATAFRAME: 0”\n[1] “——————————————————————————–”\n[1] “RECORDS AFTER DUPLICATES REMOVAL - ROWS: 1170514 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “FINAL STREET CRIME TABLE ROWS AND COLUMNS AFTER TRANSFORMATION STAGE - ROWS: 1170514 COLUMNS: 5”\n[1] “——————————————————————————–”\n[1] “TRANSFORMATION OF STREET CRIME DATAFRAME IS COMPLETED”\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1649129692411_-1719498652","id":"20220405-033452_593096632","dateCreated":"2022-04-05T03:34:52+0000","dateStarted":"2022-04-19T15:32:34+0000","dateFinished":"2022-04-19T15:37:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11546","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=341","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=342","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=343","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=344","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=345","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=346","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=347","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=348","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=349","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=350","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=351","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=352","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=353","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=354","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=355","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=356","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=357","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=358","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=359","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=360","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=361","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=362","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=363","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=364","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=365","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=366","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=367","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=368","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=369","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=370","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=371","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=372","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=373","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=374","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=375","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=376","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=377","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=378","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=379","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=380","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=381","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=382","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=383","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=384","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=385","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=386","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=387","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=388","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=389","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=390","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=391","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=392","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=393","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=394","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=395","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=396","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=397","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=398","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=399"],"interpreterSettingId":"spark2"}}},{"text":"%spark2.r\n# DATA MODELLING - CREATING SCHEMA VIA FACTS AND DIMENSIONS\n\n#-------------------------------------DIMENSION TIME-----------------------------------------------------------\n\n# Year=c(rep(c(2020), times=12), rep(c(2021), times=12), rep(c(2022), times=2))\n# Month=c(rep(1:12, times=2), rep(c(1,2), times=1))\n# Dim_Time=data.frame(Year, Month)\n\n# Dim_Time$Quarter=ifelse(Dim_Time$Month==1,1,\n#                     ifelse(Dim_Time$Month==2,1,\n#                         ifelse(Dim_Time$Month==3,1,\n#                             ifelse(Dim_Time$Month==4,2,\n#                                 ifelse(Dim_Time$Month==5,2,\n#                                     ifelse(Dim_Time$Month==6,2,\n#                                         ifelse(Dim_Time$Month==7,3,\n#                                             ifelse(Dim_Time$Month==8,3,\n#                                                 ifelse(Dim_Time$Month==9,3,\n#                                                     ifelse(Dim_Time$Month==10,4,\n#                                                         ifelse(Dim_Time$Month==11,4,4)))))))))))\n\n# Dim_Time=as.DataFrame(Dim_Time)\n# createOrReplaceTempView(Dim_Time, \"Dim_Time\")\n# Dim_Time=sql(\"SELECT Year, Quarter, Month, ROW_NUMBER() OVER(ORDER BY Year, Quarter, Month) AS Id FROM Dim_Time\")\nhead(Dim_Time,100)\n# rm(Year, Month)\n#-------------------------------------DIMENSION COUNTRIES-----------------------------------------------------------\n\n# Dim_Countries=sql(\"Select distinct country from ukc\")\n# createOrReplaceTempView(Dim_Countries, \"Dim_Countries\")\n# Dim_Countries=sql(\"SELECT country, ROW_NUMBER() OVER(ORDER BY country) AS Id FROM Dim_Countries\")\nhead(Dim_Countries)\n\n#-------------------------------------FACT CRIME UK NATION -----------------------------------------------------------\n\n# Fact_CrimeNation=sql(\"select a.NoofCrimes, b.Id from ukc a inner join Dim_Time b on a.Year=b.Year and a.Month=b.Month\")\n# inner join Dim_Countries c on a.country=c.Country\n# head(Fact_CrimeNation)\n# data Fact_CrimeNation;\n# retain ID time_id Country_id;\n# set Fact_CrimeNation;\n# ID=_N_;\n# run;\n\n# /* Question-1: Quality Check */\n# proc sql;\n# select sum('Total Crime Rate'n) from Fact_CrimeNation;\n# quit;\n\n\n# Fact_CrimeNation <- join(ukc, Dim_Time, ukc$Year==Dim_Time$Year & ukc$Month==Dim_Time$Month)\n# Fact_CrimeNation <- join(Fact_CrimeNation[], Dim_Countries, Fact_CrimeNation$Country==Dim_Countries$Country)\n\nhead(Fact_CrimeNation)\ndim(Fact_CrimeNation)","user":"anonymous","dateUpdated":"2022-04-18T15:49:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nYear Quarter Month Id\n1  2020       1     1  1\n2  2020       1     2  2\n3  2020       1     3  3\n4  2020       2     4  4\n5  2020       2     5  5\n6  2020       2     6  6\n7  2020       3     7  7\n8  2020       3     8  8\n9  2020       3     9  9\n10 2020       4    10 10\n11 2020       4    11 11\n12 2020       4    12 12\n13 2021       1     1 13\n14 2021       1     2 14\n15 2021       1     3 15\n16 2021       2     4 16\n17 2021       2     5 17\n18 2021       2     6 18\n19 2021       3     7 19\n20 2021       3     8 20\n21 2021       3     9 21\n22 2021       4    10 22\n23 2021       4    11 23\n24 2021       4    12 24\n25 2022       1     1 25\n26 2022       1     2 26\n            country Id\n1 England and Wales  1\n2   Nothern Ireland  2\n3          Scotland  3\n\nError in handleErrors(returnStatus, conn): org.apache.spark.sql.AnalysisException: cannot resolve '`b.Id`' given input columns: [a.Country, a.Month, b.Quarter, b.Month, b.Year, a.NoofCrimes, a.Year]; line 1 pos 21;\n'Project [NoofCrimes#39, 'b.Id]\n+- Join Inner, ((cast(Year#37 as double) = Year#4663) &amp;&amp; (cast(Month#38 as double) = Month#4664))\n   :- SubqueryAlias a\n   :  +- SubqueryAlias ukc\n   :     +- Project [Country#28 AS Country#36, Year#29 AS Year#37, Month#30 AS Month#38, Total Crime Rate#31 AS NoofCrimes#39]\n   :        +- Relation[Country#28,Year#29,Month#30,Total Crime Rate#31] csv\n   +- SubqueryAlias b\n      +- SubqueryAlias dim_time\n         +- LogicalRDD [Year#4663, Month#4664, Quarter#4665], false\n\n    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n    at scala.collection.immutable.List.foreach(List.scala:381)\n    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n    at scala.collection.immutable.List.map(List.scala:285)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n    at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n    at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n    at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n    at sun.reflect.GeneratedMethodAccessor214.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)\n    at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)\n    at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:40)\n    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n    at java.lang.Thread.run(Thread.java:748)\n\n\n        Country Year Month NoofCrimes Year Quarter Month Id\n\n\n1          Scotland 2020     1      16813 2020       1     1  1\n2   Nothern Ireland 2020     1      33940 2020       1     1  1\n3 England and Wales 2020     1     447406 2020       1     1  1\n4          Scotland 2020     2      15979 2020       1     2  2\n5   Nothern Ireland 2020     2      33008 2020       1     2  2\n6 England and Wales 2020     2     424962 2020       1     2  2\n            Country Id\n1          Scotland  3\n2   Nothern Ireland  2\n3 England and Wales  1\n4          Scotland  3\n5   Nothern Ireland  2\n6 England and Wales  1\n[1] 78 10\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1650092852630_373564162","id":"20220416-070732_56134319","dateCreated":"2022-04-16T07:07:32+0000","dateStarted":"2022-04-18T15:49:20+0000","dateFinished":"2022-04-18T15:49:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11547"},{"text":"%spark2.r\ncreateOrReplaceTempView(Dim_Countries, \"Dim_Countries\")\nfinal=sql(\"SELECT country, ROW_NUMBER() OVER(ORDER BY country) AS Id FROM Dim_Countries\")\nhead(final)\n\n# select CountryName, CityName, \n#       ROW_NUMBER() OVER (PARTITION BY CountryName \n#                           ORDER BY Population DESC) AS RankedPopulation\n# FROM City\n# ORDER BY CountryName, CityName;","user":"anonymous","dateUpdated":"2022-04-18T14:47:21+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n        country Id\n\n\n1 England and Wales  1\n2   Nothern Ireland  2\n3          Scotland  3\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1650292855968_10221004","id":"20220418-144055_1319672048","dateCreated":"2022-04-18T14:40:55+0000","dateStarted":"2022-04-18T14:47:21+0000","dateFinished":"2022-04-18T14:47:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11548"},{"text":"%spark2.r\n#SECTION: CROSS VALIDATION\n\n# createOrReplaceTempView(scf, \"scf\")\n# createOrReplaceTempView(ocfinal, \"ocfinal\")\n\n#CROSS VALIDATION OF OCFINAL DATA FOR DUPLICATES\n# o=sql(\"select count(distinct CrimeID) as OCCrime from ocfinal \")\n# head(o)\n# o=sql(\"select CrimeID, count(*) as OCCrime from ocfinal group by CrimeID order by OCCrime desc \")\n# head(o)\n# o=sql(\"select * from ocfinal where CrimeID = 'a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4'\")\n# head(o)\n\n\n#Records with Multiple Records in OCFINAL dataset\n#                                                       CrimeID OCCrime\n\n\n# 1 a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4       4\n# 2 a9517731095732524a5879db560047f8a4d650dbb5e3c6b6a63a0ea8ea13ee4e       3\n# 3 07f7590d79cad5046419cf272eed2a2cdfc5241ad4d2bad5137b2327e9f95736       3\n# 4 6bfded35303fa9653e48d8311c7b1db7d448bf9ea2f0c7018e711e5ab6b6edd3       3\n# 5 84c500a21b14bc0937e408ec790801129ae90c48fb4135a98723bf34bd15c667       2\n# 6 64d2d633dc2c1bd0c506a589c109420ecea329f1e016719e95118ec6ba995bf3       2\n\n#Single CrimeID has different Outcomes for same month and year\n#                                                      CrimeID Month\n\n\n# 1 a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4     1\n# 2 a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4     1\n# 3 a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4     1\n# 4 a5a6b5b322e36983b68d2d9630bdb64d9a5008245691c261030c3216701178b4     1\n#                   Outcometype Year\n# 1 Unable to prosecute suspect 2020\n# 2    Offender given a caution 2020\n# 3            Local resolution 2020\n# 4             Suspect charged 2020","user":"anonymous","dateUpdated":"2022-04-16T07:25:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1650030018077_-831814217","id":"20220415-134018_1049944424","dateCreated":"2022-04-15T13:40:18+0000","dateStarted":"2022-04-16T06:27:28+0000","dateFinished":"2022-04-16T06:29:57+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11549"},{"text":"%spark2.r\nhiveContext <- sparkRHive.init(sc)\ncreateDataFrame(hiveContext, ukc)","user":"anonymous","dateUpdated":"2022-04-16T15:23:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n<pre><code>Error in f(...): unexpected type: SparkDataFrame\n</code></pre>\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1650094979702_330816241","id":"20220416-074259_1447045602","dateCreated":"2022-04-16T07:42:59+0000","dateStarted":"2022-04-16T15:23:55+0000","dateFinished":"2022-04-16T15:23:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11550"},{"text":"%jdbc(hive)\nCREATE TABLE default.Persons (\n    PersonID int,\n    LastName varchar(255),\n    FirstName varchar(255),\n    Address varchar(255),\n    City varchar(255)\n)\n\n","user":"anonymous","dateUpdated":"2022-04-16T15:16:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"database_name":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Query executed successfully. Affected rows : -1"}]},"apps":[],"jobName":"paragraph_1650118316785_1671877363","id":"20220416-141156_1086757266","dateCreated":"2022-04-16T14:11:56+0000","dateStarted":"2022-04-16T15:16:20+0000","dateFinished":"2022-04-16T15:16:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11551"},{"text":"%jdbc(hive)\n\nCREATE TABLE default.jack AS SELECT * FROM ukc\n","user":"anonymous","dateUpdated":"2022-04-16T15:19:53+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:43 Table not found 'ukc'\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:300)\n\tat org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:286)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:324)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:265)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:718)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:801)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:633)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:43 Table not found 'ukc'\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:199)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:262)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\n\tat com.sun.proxy.$Proxy65.executeStatementAsync(Unknown Source)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\t... 3 more\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.parse.SemanticException:Line 1:43 Table not found 'ukc'\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2154)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2074)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:12158)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12252)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:358)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1863)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1810)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1805)\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n\t... 26 more\n"}]},"apps":[],"jobName":"paragraph_1650116793659_-143386359","id":"20220416-134633_732023825","dateCreated":"2022-04-16T13:46:33+0000","dateStarted":"2022-04-16T15:19:53+0000","dateFinished":"2022-04-16T15:19:53+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:11552"},{"text":"%spark2\nsqlContext.sql(\"create table default.one as select * from ukc\")\n\n","user":"anonymous","dateUpdated":"2022-04-16T15:36:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res1: org.apache.spark.sql.DataFrame = []\n"}]},"apps":[],"jobName":"paragraph_1650116872688_15374658","id":"20220416-134752_1309614607","dateCreated":"2022-04-16T13:47:52+0000","dateStarted":"2022-04-16T15:36:23+0000","dateFinished":"2022-04-16T15:36:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11553"},{"text":"%spark2.r\nwrite.df(ukc, \"/user/maria_dev/testhar/jack.csv\", \"com.databricks.spark.csv\", 'overwrite')","user":"anonymous","dateUpdated":"2022-04-16T17:56:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1650122273683_-479222749","id":"20220416-151753_1927420044","dateCreated":"2022-04-16T15:17:53+0000","dateStarted":"2022-04-16T17:56:35+0000","dateFinished":"2022-04-16T17:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11554"},{"text":"%spark2.r\n\nlocation:hdfs://sandbox-hdp.hortonworks.com:8020/warehouse/tablespace/managed/hive/jack\n","user":"anonymous","dateUpdated":"2022-04-16T15:45:56+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1650123646760_2041106736","id":"20220416-154046_1427290679","dateCreated":"2022-04-16T15:40:46+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:11555"},{"text":"%jdbc(hive)\nload data inpath '/user/maria_dev/data/testhar/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv' overwrite into table default.testhar;","user":"anonymous","dateUpdated":"2022-04-16T17:59:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Line 1:17 Invalid path ''/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv'': No files matching path hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:300)\n\tat org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:286)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:324)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:265)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.commons.dbcp2.DelegatingStatement.execute(DelegatingStatement.java:291)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.executeSql(JDBCInterpreter.java:718)\n\tat org.apache.zeppelin.jdbc.JDBCInterpreter.interpret(JDBCInterpreter.java:801)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:633)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Line 1:17 Invalid path ''/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv'': No files matching path hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:199)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:262)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\n\tat com.sun.proxy.$Proxy65.executeStatementAsync(Unknown Source)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:315)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\t... 3 more\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.parse.SemanticException:Line 1:17 Invalid path ''/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv'': No files matching path hdfs://sandbox-hdp.hortonworks.com:8020/user/maria_dev/data/testhar/jack/part-00000-6d76bede-8190-4b21-8f10-6c23946cb3d6-c000.csv\n\tat org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.applyConstraintsAndGetFiles(LoadSemanticAnalyzer.java:176)\n\tat org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeLoad(LoadSemanticAnalyzer.java:341)\n\tat org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:260)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1863)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1810)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1805)\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n\t... 26 more\n"}]},"apps":[],"jobName":"paragraph_1650130452817_860211953","id":"20220416-173412_447179166","dateCreated":"2022-04-16T17:34:12+0000","dateStarted":"2022-04-16T17:58:09+0000","dateFinished":"2022-04-16T17:58:09+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:11556"},{"text":"%spark2.r\n\ncreateOrReplaceTempView(Dim_Time, \"Dim_Time\")\nDim_Time=sql(\"SELECT Year, Quarter, Month, ROW_NUMBER() OVER(ORDER BY Year, Quarter, Month) AS Id FROM Dim_Time\")\nhead(Dim_Time)\n","user":"anonymous","dateUpdated":"2022-04-18T14:57:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"r","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/r"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nError in (function (classes, fdef, mtable) : unable to find an inherited method for function 'createOrReplaceTempView' for signature '\"data.frame\", \"character\"'\n\n\nError in handleErrors(returnStatus, conn): org.apache.spark.sql.AnalysisException: Table or view not found: Dim_Time; line 1 pos 89\n    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:665)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:617)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:647)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:640)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:640)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:586)\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n    at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n    at scala.collection.immutable.List.foldLeft(List.scala:84)\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n    at scala.collection.immutable.List.foreach(List.scala:381)\n    at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\n    at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n    at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)\n    at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)\n    at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:40)\n    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310)\n    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)\n    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)\n    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)\n    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935)\n    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)\n    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)\n    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)\n    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'dim_time' not found in database 'default';\n    at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n    at org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n    at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)\n    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:118)\n    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getRawTable$1.apply(HiveExternalCatalog.scala:118)\n    at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n    at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:117)\n    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:684)\n    at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:684)\n    at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n    at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:683)\n    at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:672)\n    at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala\n\n\nYear Month Quarter\n1 2020     1       1\n2 2020     2       1\n3 2020     3       1\n4 2020     4       2\n5 2020     5       2\n6 2020     6       2\n\n\n\n"}]},"apps":[],"jobName":"paragraph_1650130781903_199311387","id":"20220416-173941_277660987","dateCreated":"2022-04-16T17:39:41+0000","dateStarted":"2022-04-18T14:57:06+0000","dateFinished":"2022-04-18T14:57:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11557"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1650293809956_195597155","id":"20220418-145649_294608661","dateCreated":"2022-04-18T14:56:49+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:11558","text":"head(\n  summarize(\n    groupBy(ssf, ssf$Year, ssf$Month, ssf$Selfdefinedethnicity, ssf$Outcome),\n    counts = n(ssf$Year)\n  )\n)","dateUpdated":"2022-04-19T12:30:30+0000"}],"name":"ADMPGroup6_ETL","id":"2H29EQC2H","noteParams":{},"noteForms":{},"angularObjects":{"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}